{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pip install llama_index"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pip install python-dotenv"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pip install --upgrade sqlalchemy"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\r\n",
    "from dotenv import load_dotenv\r\n",
    "\r\n",
    "load_dotenv()\r\n",
    "\r\n",
    "# Now you can access the OpenAI API key using the environment variable:\r\n",
    "os.environ.get('OPENAI_API_KEY')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "from llama_index import SimpleDirectoryReader\r\n",
    "from llama_index import download_loader\r\n",
    "\r\n",
    "# Downloads the document to be used in the chatbot\r\n",
    "SimpleDirectoryReader = download_loader(\"SimpleDirectoryReader\")\r\n",
    "loader = SimpleDirectoryReader('./data', recursive=True, exclude_hidden=True)\r\n",
    "documents = loader.load_data()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# Import necessary classes from the LlamaIndex library\r\n",
    "from llama_index import LLMPredictor, GPTVectorStoreIndex, PromptHelper, ServiceContext\r\n",
    "# Import to OpenAI class from langchain library\r\n",
    "from langchain import OpenAI\r\n",
    "\r\n",
    "# Defines the language model for the LLMPredictor\r\n",
    "# In this case, it is an OpenAI template with temperature 0 and name \"text-davinci-003\".\r\n",
    "llm_predictor = LLMPredictor(llm=OpenAI(temperature=0, model_name=\"text-davinci-003\"))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# sets PromptHelper parameters\r\n",
    "# The maximum input size for the prompt is 4096 characters\r\n",
    "# The maximum output number is 256 tokens\r\n",
    "# The maximum chunk overlap is 20 characters\r\n",
    "max_input_size = 4096\r\n",
    "num_output = 256\r\n",
    "max_chunk_overlap = 20\r\n",
    "prompt_helper = PromptHelper(max_input_size, num_output, max_chunk_overlap)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# Defines the ServiceContext to encapsulate the prediction and prompt logic\r\n",
    "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, prompt_helper=prompt_helper)\r\n",
    "\r\n",
    "# Creates a GPTSimpleVectorIndex from the documents to be indexed\r\n",
    "# Uses LLMPredictor text prediction to create a simple vector representation of the documents\r\n",
    "# Uses the ServiceContext to encapsulate the prediction and prompt logic\r\n",
    "index = GPTVectorStoreIndex.from_documents(\r\n",
    "    documents, service_context=service_context\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from langchain.callbacks import get_openai_callback\r\n",
    "\r\n",
    "# Creates the query engine\r\n",
    "query_engine = index.as_query_engine()\r\n",
    "\r\n",
    "# Make the query using the example query string\r\n",
    "with get_openai_callback() as cb:\r\n",
    "    query_str = \"How can I open the tailgate?\"\r\n",
    "    response = query_engine.query(query_str)\r\n",
    "    print(cb)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "formatted_response = response.response.replace('\\n\\n', '\\n')\r\n",
    "print(formatted_response)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.2 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "interpreter": {
   "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}